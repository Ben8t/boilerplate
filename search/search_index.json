{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"analytics/01-exploration/","text":"Exploration Read data with Pandas import pandas as pd import numpy as np data_file = \"...\" data = pd . read_csv ( data_file ) num_columns = len ( data . columns ) pd . set_option ( \"display.max_columns\" , num_columns ) data . head ( 30 ) Plot value counts import seaborn as sns titanic = sns . load_dataset ( \"titanic\" ) ax = sns . countplot ( x = \"class\" , data = titanic )","title":"Exploration"},{"location":"analytics/01-exploration/#exploration","text":"","title":"Exploration"},{"location":"analytics/01-exploration/#read-data-with-pandas","text":"import pandas as pd import numpy as np data_file = \"...\" data = pd . read_csv ( data_file ) num_columns = len ( data . columns ) pd . set_option ( \"display.max_columns\" , num_columns ) data . head ( 30 )","title":"Read data with Pandas"},{"location":"analytics/01-exploration/#plot-value-counts","text":"import seaborn as sns titanic = sns . load_dataset ( \"titanic\" ) ax = sns . countplot ( x = \"class\" , data = titanic )","title":"Plot value counts"},{"location":"analytics/02-transform/","text":"Transform Pandas group-by import pandas as pd data = pd . read_csv ( 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv' ) transformed_data = ( data . groupby ([ \"species\" ]) . agg ( mean_sepal_length = ( \"sepal_length\" , \"mean\" ), mean_sepal_width = ( \"sepal_width\" , \"mean\" ), max_petal_length = ( \"petal_length\" , \"max\" ) ) . reset_index () ) Pandas JSON Normalize import pandas as pd data = [ { \"category\" : \"car\" , \"release_date\" : 2012 , \"assets\" : [ { \"key\" : \"seat\" , \"value\" : 4 , \"quality\" : 3 }, { \"key\" : \"seat\" , \"value\" : 6 , \"quality\" : 9 }, { \"key\" : \"gear\" , \"value\" : 4 , \"quality\" : 1 } ] }, { \"category\" : \"truck\" , \"release_date\" : 2018 , \"assets\" : [ { \"key\" : \"wheels\" , \"value\" : 6 , \"quality\" : 2 }, { \"key\" : \"seat\" , \"value\" : 3 , \"quality\" : 10 }, { \"key\" : \"gear\" , \"value\" : 6 , \"quality\" : 8 } ] } ] pd . json_normalize ( data , \"assets\" , [ \"category\" , \"release_date\" ]) # key value quality category release_date # 0 seat 4 3 car 2012 # 1 seat 6 9 car 2012 # 2 gear 4 1 car 2012 # 3 wheels 6 2 truck 2018 # 4 seat 3 10 truck 2018 # 5 gear 6 8 truck 2018","title":"Transform"},{"location":"analytics/02-transform/#transform","text":"","title":"Transform"},{"location":"analytics/02-transform/#pandas-group-by","text":"import pandas as pd data = pd . read_csv ( 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv' ) transformed_data = ( data . groupby ([ \"species\" ]) . agg ( mean_sepal_length = ( \"sepal_length\" , \"mean\" ), mean_sepal_width = ( \"sepal_width\" , \"mean\" ), max_petal_length = ( \"petal_length\" , \"max\" ) ) . reset_index () )","title":"Pandas group-by"},{"location":"analytics/02-transform/#pandas-json-normalize","text":"import pandas as pd data = [ { \"category\" : \"car\" , \"release_date\" : 2012 , \"assets\" : [ { \"key\" : \"seat\" , \"value\" : 4 , \"quality\" : 3 }, { \"key\" : \"seat\" , \"value\" : 6 , \"quality\" : 9 }, { \"key\" : \"gear\" , \"value\" : 4 , \"quality\" : 1 } ] }, { \"category\" : \"truck\" , \"release_date\" : 2018 , \"assets\" : [ { \"key\" : \"wheels\" , \"value\" : 6 , \"quality\" : 2 }, { \"key\" : \"seat\" , \"value\" : 3 , \"quality\" : 10 }, { \"key\" : \"gear\" , \"value\" : 6 , \"quality\" : 8 } ] } ] pd . json_normalize ( data , \"assets\" , [ \"category\" , \"release_date\" ]) # key value quality category release_date # 0 seat 4 3 car 2012 # 1 seat 6 9 car 2012 # 2 gear 4 1 car 2012 # 3 wheels 6 2 truck 2018 # 4 seat 3 10 truck 2018 # 5 gear 6 8 truck 2018","title":"Pandas JSON Normalize"},{"location":"analytics/03-datetime/","text":"Date & Time Get formatted today date import datetime date = datetime . datetime . today () formatted_date = date . strftime ( \"%Y%m %d \" ) Parse date string If your string is already in iso-format ( %Y-%m-%d ) from datetime import date date_variable = date . fromisoformat ( '2019-12-04' ) if you have other format from datetime import datetime new_date = datetime . strptime ( '20190109' , '%Y%m %d ' ) or from dateutil import parser datetime_obj = parser . parse ( '2018-02-06T13:12:18.1278015Z' ) print datetime_obj # output: datetime.datetime(2018, 2, 6, 13, 12, 18, 127801, tzinfo=tzutc()) Can be slow... Convert Pandas string to datetime import pandas as pd import datetime data = pd . DataFrame ([{ \"str_date\" : \"20210101\" }, { \"str_date\" : \"20210201\" }]) parsed_data = ( data . assign ( date = lambda x : pd . to_datetime ( x [ \"str_date\" ], format = \"%Y%m %d \" , errors = \"ignore\" )) ) Manipulate date with Operation Remove 120 days from a given date : operation date -d 2021 01 01 0 0 120 --sub Add 2 years, 1 month and 15 days to a given date operation date -d 2021 01 01 2 1 15 --add Create range of date import pandas as pd pd . date_range ( '2021-02-01' , '2021-12-01' , freq = 'MS' ) . strftime ( \"%Y%m %d \" ) . tolist ()","title":"Date & Time"},{"location":"analytics/03-datetime/#date-time","text":"","title":"Date &amp; Time"},{"location":"analytics/03-datetime/#get-formatted-today-date","text":"import datetime date = datetime . datetime . today () formatted_date = date . strftime ( \"%Y%m %d \" )","title":"Get formatted today date"},{"location":"analytics/03-datetime/#parse-date-string","text":"If your string is already in iso-format ( %Y-%m-%d ) from datetime import date date_variable = date . fromisoformat ( '2019-12-04' ) if you have other format from datetime import datetime new_date = datetime . strptime ( '20190109' , '%Y%m %d ' ) or from dateutil import parser datetime_obj = parser . parse ( '2018-02-06T13:12:18.1278015Z' ) print datetime_obj # output: datetime.datetime(2018, 2, 6, 13, 12, 18, 127801, tzinfo=tzutc()) Can be slow...","title":"Parse date string"},{"location":"analytics/03-datetime/#convert-pandas-string-to-datetime","text":"import pandas as pd import datetime data = pd . DataFrame ([{ \"str_date\" : \"20210101\" }, { \"str_date\" : \"20210201\" }]) parsed_data = ( data . assign ( date = lambda x : pd . to_datetime ( x [ \"str_date\" ], format = \"%Y%m %d \" , errors = \"ignore\" )) )","title":"Convert Pandas string to datetime"},{"location":"analytics/03-datetime/#manipulate-date-with-operation","text":"Remove 120 days from a given date : operation date -d 2021 01 01 0 0 120 --sub Add 2 years, 1 month and 15 days to a given date operation date -d 2021 01 01 2 1 15 --add","title":"Manipulate date with Operation"},{"location":"analytics/03-datetime/#create-range-of-date","text":"import pandas as pd pd . date_range ( '2021-02-01' , '2021-12-01' , freq = 'MS' ) . strftime ( \"%Y%m %d \" ) . tolist ()","title":"Create range of date"},{"location":"analytics/04-data-vizualisation/","text":"Data-Vizualisation Custom ggplot theme custom_theme <- function (){ list ( theme ( plot.margin = unit ( c ( 1 , 2 , 1 , 1 ), \"cm\" )), theme ( panel.grid.minor = element_line ( color = \"#DCDCDC\" ), panel.grid.major = element_line ( color = \"#DCDCDC\" ), panel.border = element_blank (), panel.background = element_blank (), axis.ticks = element_line (), axis.line.x = element_line ( size = 0.2 , linetype = \"solid\" , colour = \"black\" ), axis.line.y = element_line ( size = 0.2 , linetype = \"solid\" , colour = \"black\" ), axis.text.x = element_text ( size = 15 ), axis.text.y = element_text ( size = 15 ), axis.title.x = element_text ( size = 20 ), axis.title.y = element_text ( size = 20 ), legend.title = element_text ( size = 20 ), legend.text = element_text ( size = 20 ) ), theme ( text = element_text ( family = \"Object Sans\" , face = \"bold\" , size = 12 )) ) } Save plot ggsave ( plot , bg = \"white\" , width = 30 , height = 20 , units = \"cm\" , dpi = 300 ) Basic Plotnine (Python) from plotnine import ggplot , geom_point , geom_line , labs , ggsave plot = ( ggplot ( data , aes ( x = \"x\" , y = \"y\" , color = \"color\" )) + geom_line () + labs ( x = \"x_label\" ) ) ggsave ( plot , \"plot.png\" ) Plot distribution differences between two value import numpy as np import pandas as pd from plotnine import ggplot , aes , labs , aes , geom_histogram , theme_minimal , scale_fill_manual , scale_x_continuous data = pd . DataFrame ({ \"column_a\" : np . random . randint ( 0 , 100 , 5000 ), \"column_b\" : np . random . randint ( 0 , 100 , 5000 )}) melted_data = ( data . melt ( id_vars = [], value_vars = [ \"column_a\" , \"column_b\" ]) ) ( ggplot ( melted_data , aes ( x = \"value\" , fill = \"variable\" )) + geom_histogram ( binwidth = 10 , position = \"dodge\" , color = \"white\" ) + scale_fill_manual ( values = [ '#27a3e0' , '#0740af' ], labels = [ \"A\" , \"B\" ]) + scale_x_continuous ( breaks = [ 0 , 50 , 100 ], expand = ( 0 , 0 )) + labs ( x = \"Value\" , y = \"Count\" , fill = \"Two variables\" ) + theme_minimal () )","title":"Data-Vizualisation"},{"location":"analytics/04-data-vizualisation/#data-vizualisation","text":"","title":"Data-Vizualisation"},{"location":"analytics/04-data-vizualisation/#custom-ggplot-theme","text":"custom_theme <- function (){ list ( theme ( plot.margin = unit ( c ( 1 , 2 , 1 , 1 ), \"cm\" )), theme ( panel.grid.minor = element_line ( color = \"#DCDCDC\" ), panel.grid.major = element_line ( color = \"#DCDCDC\" ), panel.border = element_blank (), panel.background = element_blank (), axis.ticks = element_line (), axis.line.x = element_line ( size = 0.2 , linetype = \"solid\" , colour = \"black\" ), axis.line.y = element_line ( size = 0.2 , linetype = \"solid\" , colour = \"black\" ), axis.text.x = element_text ( size = 15 ), axis.text.y = element_text ( size = 15 ), axis.title.x = element_text ( size = 20 ), axis.title.y = element_text ( size = 20 ), legend.title = element_text ( size = 20 ), legend.text = element_text ( size = 20 ) ), theme ( text = element_text ( family = \"Object Sans\" , face = \"bold\" , size = 12 )) ) }","title":"Custom ggplot theme"},{"location":"analytics/04-data-vizualisation/#save-plot","text":"ggsave ( plot , bg = \"white\" , width = 30 , height = 20 , units = \"cm\" , dpi = 300 )","title":"Save plot"},{"location":"analytics/04-data-vizualisation/#basic-plotnine-python","text":"from plotnine import ggplot , geom_point , geom_line , labs , ggsave plot = ( ggplot ( data , aes ( x = \"x\" , y = \"y\" , color = \"color\" )) + geom_line () + labs ( x = \"x_label\" ) ) ggsave ( plot , \"plot.png\" )","title":"Basic Plotnine (Python)"},{"location":"analytics/04-data-vizualisation/#plot-distribution-differences-between-two-value","text":"import numpy as np import pandas as pd from plotnine import ggplot , aes , labs , aes , geom_histogram , theme_minimal , scale_fill_manual , scale_x_continuous data = pd . DataFrame ({ \"column_a\" : np . random . randint ( 0 , 100 , 5000 ), \"column_b\" : np . random . randint ( 0 , 100 , 5000 )}) melted_data = ( data . melt ( id_vars = [], value_vars = [ \"column_a\" , \"column_b\" ]) ) ( ggplot ( melted_data , aes ( x = \"value\" , fill = \"variable\" )) + geom_histogram ( binwidth = 10 , position = \"dodge\" , color = \"white\" ) + scale_fill_manual ( values = [ '#27a3e0' , '#0740af' ], labels = [ \"A\" , \"B\" ]) + scale_x_continuous ( breaks = [ 0 , 50 , 100 ], expand = ( 0 , 0 )) + labs ( x = \"Value\" , y = \"Count\" , fill = \"Two variables\" ) + theme_minimal () )","title":"Plot distribution differences between two value"},{"location":"machine-learning/01-mlflow/","text":"MLFlow Basic experiment import mlflow # import mlflow.sklearn with mlflow . start_run (): mlflow . log_params ({ \"param1\" : param1 , \"param2\" : param2 }) mlflow . log_param ( \"features\" , features ) begin_time = time . time () data = get_data ( ... ) model = train_model ( ... ) end_time = time . time () mlflow . log_metric ( \"training_time\" , end_time - begin_time ) metric = evaluate ( model , data ) mlflow . log_metric ( \"metric\" : metric ) # save_model(model, f'./artifacts/models/model.pkl') # mlflow.sklearn.log_model(model, \"model\")","title":"MLFlow"},{"location":"machine-learning/01-mlflow/#mlflow","text":"","title":"MLFlow"},{"location":"machine-learning/01-mlflow/#basic-experiment","text":"import mlflow # import mlflow.sklearn with mlflow . start_run (): mlflow . log_params ({ \"param1\" : param1 , \"param2\" : param2 }) mlflow . log_param ( \"features\" , features ) begin_time = time . time () data = get_data ( ... ) model = train_model ( ... ) end_time = time . time () mlflow . log_metric ( \"training_time\" , end_time - begin_time ) metric = evaluate ( model , data ) mlflow . log_metric ( \"metric\" : metric ) # save_model(model, f'./artifacts/models/model.pkl') # mlflow.sklearn.log_model(model, \"model\")","title":"Basic experiment"},{"location":"machine-learning/02-scikit-learn/","text":"Scikit-Learn Split dataset from sklearn.model_selection import train_test_split split_size = 0.3 target = [ ... ] x = data . drop ( target , axis = 1 ) y = data [ target ] x_train , x_validation , y_train , y_validation = train_test_split ( x , y , test_size = split_size ) Metric evaluation Regression import numpy as np from sklearn.metrics import mean_absolute_error , mean_squared_error , r2_score def compute_regression_metrics ( y_true , y_pred ): return { \"mae\" : mean_absolute_error ( y_true , y_pred ), \"rmse\" : np . sqrt ( mean_squared_error ( y_true , y_pred )), \"r2\" : r2_score ( y_true , y_pred ) } Example: y_true = [ 1 , 2 , 3 , 4 , 5 ] y_pred = [ 0.9 , 2 , 3 , 4.2 , 5.6 ] regression_metrics = compute_regression_metrics ( y_true , y_pred ) mae = regression_metrics . get ( \"mae\" ) rmse = regression_metrics . get ( \"rmse\" ) r2 = regression_metrics . get ( \"r2\" ) Classification from sklearn.metrics import accuracy_score , precision_score , recall_score def compute_classification_metrics ( y_true , y_pred ): return { \"accuracy\" : accuracy_score ( y_true , y_pred ), \"precision\" : precision_score ( y_true , y_pred ), \"recall\" : recall_score ( y_true , y_pred ) } Example: y_true = [ 1 , 0 , 1 , 1 , 0 ] y_pred = [ 0 , 0 , 1 , 0 , 0 ] classification_metrics = compute_classification_metrics ( y_true , y_pred ) accuracy = classification_metrics . get ( \"accuracy\" ) precision = classification_metrics . get ( \"precision\" ) recall = classification_metrics . get ( \"recall\" ) Data Preprocessing Pipeline Example for numeric and categorical features set : from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler , OneHotEncoder def build_preprocessor ( numeric_features : list , categorical_features : list ) -> ColumnTransformer : numeric_transformer = Pipeline ( steps = [( \"scaler\" , SimpleImputer ())]) categorical_transformer = Pipeline ( steps = [ ( \"nan_resolve\" , SimpleImputer ( strategy = \"constant\" , fill_value = 0 )), ( \"onehot\" , OneHotEncoder ( handle_unknown = \"ignore\" )) ] ) preprocessor = ColumnTransformer ( transformers = [ ( \"numeric\" , numeric_transformer , numeric_features ), ( \"categorical\" , categorical_transformer , categorical_features ) ] ) return preprocessor # numeric_features, categorical_features = [...], [...] # preprocessor = build_preprocessor(numeric_features, categorical_features) # classifier = RandomForestClassifier() # model = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", classifier)])","title":"Scikit-Learn"},{"location":"machine-learning/02-scikit-learn/#scikit-learn","text":"","title":"Scikit-Learn"},{"location":"machine-learning/02-scikit-learn/#split-dataset","text":"from sklearn.model_selection import train_test_split split_size = 0.3 target = [ ... ] x = data . drop ( target , axis = 1 ) y = data [ target ] x_train , x_validation , y_train , y_validation = train_test_split ( x , y , test_size = split_size )","title":"Split dataset"},{"location":"machine-learning/02-scikit-learn/#metric-evaluation","text":"","title":"Metric evaluation"},{"location":"machine-learning/02-scikit-learn/#regression","text":"import numpy as np from sklearn.metrics import mean_absolute_error , mean_squared_error , r2_score def compute_regression_metrics ( y_true , y_pred ): return { \"mae\" : mean_absolute_error ( y_true , y_pred ), \"rmse\" : np . sqrt ( mean_squared_error ( y_true , y_pred )), \"r2\" : r2_score ( y_true , y_pred ) } Example: y_true = [ 1 , 2 , 3 , 4 , 5 ] y_pred = [ 0.9 , 2 , 3 , 4.2 , 5.6 ] regression_metrics = compute_regression_metrics ( y_true , y_pred ) mae = regression_metrics . get ( \"mae\" ) rmse = regression_metrics . get ( \"rmse\" ) r2 = regression_metrics . get ( \"r2\" )","title":"Regression"},{"location":"machine-learning/02-scikit-learn/#classification","text":"from sklearn.metrics import accuracy_score , precision_score , recall_score def compute_classification_metrics ( y_true , y_pred ): return { \"accuracy\" : accuracy_score ( y_true , y_pred ), \"precision\" : precision_score ( y_true , y_pred ), \"recall\" : recall_score ( y_true , y_pred ) } Example: y_true = [ 1 , 0 , 1 , 1 , 0 ] y_pred = [ 0 , 0 , 1 , 0 , 0 ] classification_metrics = compute_classification_metrics ( y_true , y_pred ) accuracy = classification_metrics . get ( \"accuracy\" ) precision = classification_metrics . get ( \"precision\" ) recall = classification_metrics . get ( \"recall\" )","title":"Classification"},{"location":"machine-learning/02-scikit-learn/#data-preprocessing-pipeline","text":"Example for numeric and categorical features set : from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler , OneHotEncoder def build_preprocessor ( numeric_features : list , categorical_features : list ) -> ColumnTransformer : numeric_transformer = Pipeline ( steps = [( \"scaler\" , SimpleImputer ())]) categorical_transformer = Pipeline ( steps = [ ( \"nan_resolve\" , SimpleImputer ( strategy = \"constant\" , fill_value = 0 )), ( \"onehot\" , OneHotEncoder ( handle_unknown = \"ignore\" )) ] ) preprocessor = ColumnTransformer ( transformers = [ ( \"numeric\" , numeric_transformer , numeric_features ), ( \"categorical\" , categorical_transformer , categorical_features ) ] ) return preprocessor # numeric_features, categorical_features = [...], [...] # preprocessor = build_preprocessor(numeric_features, categorical_features) # classifier = RandomForestClassifier() # model = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", classifier)])","title":"Data Preprocessing Pipeline"},{"location":"ops/aws/","text":"AWS Load JSON file from S3 import json import boto3 s3_client = boto3 . client ( 's3' ) def get_json_data ( s3_client : object , bucket : str , key : str ) -> dict or None : try : obj = s3_client . get_object ( Bucket = bucket , Key = key ) return json . loads ( obj . get ( 'Body' ) . read ()) except ClientError as e : if e . response [ 'Error' ][ 'Code' ] == 'NoSuchKey' : return None else : raise ValueError ( e )","title":"AWS"},{"location":"ops/aws/#aws","text":"","title":"AWS"},{"location":"ops/aws/#load-json-file-from-s3","text":"import json import boto3 s3_client = boto3 . client ( 's3' ) def get_json_data ( s3_client : object , bucket : str , key : str ) -> dict or None : try : obj = s3_client . get_object ( Bucket = bucket , Key = key ) return json . loads ( obj . get ( 'Body' ) . read ()) except ClientError as e : if e . response [ 'Error' ][ 'Code' ] == 'NoSuchKey' : return None else : raise ValueError ( e )","title":"Load JSON file from S3"},{"location":"ops/docker/","text":"Docker Basic Python Dockerfile FROM python:3.7 WORKDIR /app COPY requirements.txt ./requirements.txt RUN pip install -r requirements.txt COPY ./app.py /app/app.py CMD [ \"python\" , \"./app.py\" ] Common commands or flags Buildkit (enable experimental features). DOCKER_BUILDKIT = 1 docker build ... Build with SSH key. Useful for cloning privte GitHub repository. --ssh github_ssh_key = /Users/ $USER /.ssh/id_rsa Quickly add AWS credentials in volume. -v /Users/ $USER /.aws/credentials:/root/.aws/credentials Add environment file. --env-file .env","title":"Docker"},{"location":"ops/docker/#docker","text":"","title":"Docker"},{"location":"ops/docker/#basic-python-dockerfile","text":"FROM python:3.7 WORKDIR /app COPY requirements.txt ./requirements.txt RUN pip install -r requirements.txt COPY ./app.py /app/app.py CMD [ \"python\" , \"./app.py\" ]","title":"Basic Python Dockerfile"},{"location":"ops/docker/#common-commands-or-flags","text":"Buildkit (enable experimental features). DOCKER_BUILDKIT = 1 docker build ... Build with SSH key. Useful for cloning privte GitHub repository. --ssh github_ssh_key = /Users/ $USER /.ssh/id_rsa Quickly add AWS credentials in volume. -v /Users/ $USER /.aws/credentials:/root/.aws/credentials Add environment file. --env-file .env","title":"Common commands or flags"},{"location":"ops/git/","text":"Git Work on different branch simultaneously git worktree add <../folder> -b <branch> It's better to add the .. to the folder path to be seperate from the base repository (you end up with two repositories poiting to two different branches).","title":"Git"},{"location":"ops/git/#git","text":"","title":"Git"},{"location":"ops/git/#work-on-different-branch-simultaneously","text":"git worktree add <../folder> -b <branch> It's better to add the .. to the folder path to be seperate from the base repository (you end up with two repositories poiting to two different branches).","title":"Work on different branch simultaneously"},{"location":"ops/terminal/","text":"Terminal Screen Print all existing screens screen -ls Create a screen screen -S <screen_name> Enter an existing screen screen -r screen_name Quit a screen CTRL+A+D","title":"Terminal"},{"location":"ops/terminal/#terminal","text":"","title":"Terminal"},{"location":"ops/terminal/#screen","text":"Print all existing screens screen -ls Create a screen screen -S <screen_name> Enter an existing screen screen -r screen_name Quit a screen CTRL+A+D","title":"Screen"},{"location":"utils/01-io/","text":"I/O Read template file Python from jinja2 import Template def read_template ( template_file : str ) -> Template : with open ( template_file , \"r\" ) as template_open : return Template ( template_open . read ()) template = read_template ( \"folder/file\" ) param = 123 data = template . render ( param = param ) Jinja documentation R library ( tidyverse ) param <- 123 data <- readr :: read_file ( \"folder/file\" ) %>% glue ( . , param = param ) Read many data files Python import glob import json files = glob . glob ( \"./*.json\" ) # get all json files in currenty directory def read_file ( filepath : str ) -> dict : with open ( filepath , \"r\" ) as fopen : return json . load ( fopen ) data = [ read_file ( item ) for item in files ]","title":"I/O"},{"location":"utils/01-io/#io","text":"","title":"I/O"},{"location":"utils/01-io/#read-template-file","text":"Python from jinja2 import Template def read_template ( template_file : str ) -> Template : with open ( template_file , \"r\" ) as template_open : return Template ( template_open . read ()) template = read_template ( \"folder/file\" ) param = 123 data = template . render ( param = param ) Jinja documentation R library ( tidyverse ) param <- 123 data <- readr :: read_file ( \"folder/file\" ) %>% glue ( . , param = param )","title":"Read template file"},{"location":"utils/01-io/#read-many-data-files","text":"Python import glob import json files = glob . glob ( \"./*.json\" ) # get all json files in currenty directory def read_file ( filepath : str ) -> dict : with open ( filepath , \"r\" ) as fopen : return json . load ( fopen ) data = [ read_file ( item ) for item in files ]","title":"Read many data files"},{"location":"utils/02-unit-test/","text":"Unit Test Pytest fixture import pytest @pytest . fixture def fake_data (): return [ 1 , 2 , 3 , 4 , 5 ] def test_data_length ( fake_data ): assert len ( fake_data ) == 5 Mocker def test_with_mock ( mocker ): mocker . patch . object ( < object > , \"<object_method>\" , auto_spec = True ) handler = < object > ( params ) assert handler ...","title":"Unit Test"},{"location":"utils/02-unit-test/#unit-test","text":"","title":"Unit Test"},{"location":"utils/02-unit-test/#pytest-fixture","text":"import pytest @pytest . fixture def fake_data (): return [ 1 , 2 , 3 , 4 , 5 ] def test_data_length ( fake_data ): assert len ( fake_data ) == 5","title":"Pytest fixture"},{"location":"utils/02-unit-test/#mocker","text":"def test_with_mock ( mocker ): mocker . patch . object ( < object > , \"<object_method>\" , auto_spec = True ) handler = < object > ( params ) assert handler ...","title":"Mocker"},{"location":"utils/03-pipeline/","text":"Pipeline Build chunks def build_chunks ( items : list , batch_size : int ) -> list : \"\"\"Split data into chunks. Args: items (list): list of items to split. batch_size (int): batch size. Returns: (list): list of splitted elements (through a generator). \"\"\" for i in range ( 0 , len ( items ), batch_size ): yield items [ i : i + batch_size ]","title":"Pipeline"},{"location":"utils/03-pipeline/#pipeline","text":"","title":"Pipeline"},{"location":"utils/03-pipeline/#build-chunks","text":"def build_chunks ( items : list , batch_size : int ) -> list : \"\"\"Split data into chunks. Args: items (list): list of items to split. batch_size (int): batch size. Returns: (list): list of splitted elements (through a generator). \"\"\" for i in range ( 0 , len ( items ), batch_size ): yield items [ i : i + batch_size ]","title":"Build chunks"},{"location":"utils/04-helpers/","text":"Helpers Flatten list of lists def flat_list ( lists : list ) -> list : return [ item for sublist in lists for item in sublist ] Logging import logging logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) logger . addHandler ( logging . StreamHandler ()) Script arguments import argparse parser = argparse . ArgumentParser ( description = \"Some description\" ) parser . add_argument ( \"--some_flag\" , default = False , action = \"store_true\" ) parse . add_argument ( \"--another_flag\" , type = \"integer\" ) args = parser . parse_args () Fake Pandas Dataframe import numpy as np import pandas as pd import string row_num = 10000 col_num = 5 col_names = list ( string . ascii_uppercase )[ 0 : col_num ] df = pd . DataFrame ( np . random . randint ( 0 , row_num , size = ( row_num , col_num )), columns = col_names ) Import subdirectory in Jupyter Notebook import os import sys import inspect currentdir = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) parentdir = os . path . dirname ( currentdir ) sys . path . insert ( 0 , parentdir )","title":"Helpers"},{"location":"utils/04-helpers/#helpers","text":"","title":"Helpers"},{"location":"utils/04-helpers/#flatten-list-of-lists","text":"def flat_list ( lists : list ) -> list : return [ item for sublist in lists for item in sublist ]","title":"Flatten list of lists"},{"location":"utils/04-helpers/#logging","text":"import logging logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) logger . addHandler ( logging . StreamHandler ())","title":"Logging"},{"location":"utils/04-helpers/#script-arguments","text":"import argparse parser = argparse . ArgumentParser ( description = \"Some description\" ) parser . add_argument ( \"--some_flag\" , default = False , action = \"store_true\" ) parse . add_argument ( \"--another_flag\" , type = \"integer\" ) args = parser . parse_args ()","title":"Script arguments"},{"location":"utils/04-helpers/#fake-pandas-dataframe","text":"import numpy as np import pandas as pd import string row_num = 10000 col_num = 5 col_names = list ( string . ascii_uppercase )[ 0 : col_num ] df = pd . DataFrame ( np . random . randint ( 0 , row_num , size = ( row_num , col_num )), columns = col_names )","title":"Fake Pandas Dataframe"},{"location":"utils/04-helpers/#import-subdirectory-in-jupyter-notebook","text":"import os import sys import inspect currentdir = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) parentdir = os . path . dirname ( currentdir ) sys . path . insert ( 0 , parentdir )","title":"Import subdirectory in Jupyter Notebook"}]}